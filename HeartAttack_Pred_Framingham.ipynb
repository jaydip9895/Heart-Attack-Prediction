{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaydip/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('framingham.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4233</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>25.97</td>\n",
       "      <td>66.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>126.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>19.71</td>\n",
       "      <td>65.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>84.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>126.5</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.16</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>133.5</td>\n",
       "      <td>83.0</td>\n",
       "      <td>21.47</td>\n",
       "      <td>80.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4238 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      male  age  education  currentSmoker  cigsPerDay  BPMeds  \\\n",
       "0        1   39        4.0              0         0.0     0.0   \n",
       "1        0   46        2.0              0         0.0     0.0   \n",
       "2        1   48        1.0              1        20.0     0.0   \n",
       "3        0   61        3.0              1        30.0     0.0   \n",
       "4        0   46        3.0              1        23.0     0.0   \n",
       "...    ...  ...        ...            ...         ...     ...   \n",
       "4233     1   50        1.0              1         1.0     0.0   \n",
       "4234     1   51        3.0              1        43.0     0.0   \n",
       "4235     0   48        2.0              1        20.0     NaN   \n",
       "4236     0   44        1.0              1        15.0     0.0   \n",
       "4237     0   52        2.0              0         0.0     0.0   \n",
       "\n",
       "      prevalentStroke  prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  \\\n",
       "0                   0             0         0    195.0  106.0   70.0  26.97   \n",
       "1                   0             0         0    250.0  121.0   81.0  28.73   \n",
       "2                   0             0         0    245.0  127.5   80.0  25.34   \n",
       "3                   0             1         0    225.0  150.0   95.0  28.58   \n",
       "4                   0             0         0    285.0  130.0   84.0  23.10   \n",
       "...               ...           ...       ...      ...    ...    ...    ...   \n",
       "4233                0             1         0    313.0  179.0   92.0  25.97   \n",
       "4234                0             0         0    207.0  126.5   80.0  19.71   \n",
       "4235                0             0         0    248.0  131.0   72.0  22.00   \n",
       "4236                0             0         0    210.0  126.5   87.0  19.16   \n",
       "4237                0             0         0    269.0  133.5   83.0  21.47   \n",
       "\n",
       "      heartRate  glucose  TenYearCHD  \n",
       "0          80.0     77.0           0  \n",
       "1          95.0     76.0           0  \n",
       "2          75.0     70.0           0  \n",
       "3          65.0    103.0           1  \n",
       "4          85.0     85.0           0  \n",
       "...         ...      ...         ...  \n",
       "4233       66.0     86.0           1  \n",
       "4234       65.0     68.0           0  \n",
       "4235       84.0     86.0           0  \n",
       "4236       86.0      NaN           0  \n",
       "4237       80.0    107.0           0  \n",
       "\n",
       "[4238 rows x 16 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'male':'gender'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4238 entries, 0 to 4237\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   gender           4238 non-null   int64  \n",
      " 1   age              4238 non-null   int64  \n",
      " 2   education        4133 non-null   float64\n",
      " 3   currentSmoker    4238 non-null   int64  \n",
      " 4   cigsPerDay       4209 non-null   float64\n",
      " 5   BPMeds           4185 non-null   float64\n",
      " 6   prevalentStroke  4238 non-null   int64  \n",
      " 7   prevalentHyp     4238 non-null   int64  \n",
      " 8   diabetes         4238 non-null   int64  \n",
      " 9   totChol          4188 non-null   float64\n",
      " 10  sysBP            4238 non-null   float64\n",
      " 11  diaBP            4238 non-null   float64\n",
      " 12  BMI              4219 non-null   float64\n",
      " 13  heartRate        4237 non-null   float64\n",
      " 14  glucose          3850 non-null   float64\n",
      " 15  TenYearCHD       4238 non-null   int64  \n",
      "dtypes: float64(9), int64(7)\n",
      "memory usage: 529.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   5.,   0.,  18.,   0.,  42.,   0.,  84.,   0.,  92.,   0.,\n",
       "        144., 169.,   0., 191.,   0., 174.,   0., 180.,   0., 159.,   0.,\n",
       "        166., 162.,   0., 182.,   0., 141.,   0., 173.,   0., 132.,   0.,\n",
       "        140.,   0., 146., 149.,   0., 139.,   0., 132.,   0., 145.,   0.,\n",
       "        123.,   0., 123., 117.,   0., 119.,   0., 111.,   0., 110.,   0.,\n",
       "         99.,   0., 110.,  93.,   0.,  57.,   0.,  38.,   0.,  45.,   0.,\n",
       "         18.,   0.,   7.,   2.]),\n",
       " array([32.        , 32.54285714, 33.08571429, 33.62857143, 34.17142857,\n",
       "        34.71428571, 35.25714286, 35.8       , 36.34285714, 36.88571429,\n",
       "        37.42857143, 37.97142857, 38.51428571, 39.05714286, 39.6       ,\n",
       "        40.14285714, 40.68571429, 41.22857143, 41.77142857, 42.31428571,\n",
       "        42.85714286, 43.4       , 43.94285714, 44.48571429, 45.02857143,\n",
       "        45.57142857, 46.11428571, 46.65714286, 47.2       , 47.74285714,\n",
       "        48.28571429, 48.82857143, 49.37142857, 49.91428571, 50.45714286,\n",
       "        51.        , 51.54285714, 52.08571429, 52.62857143, 53.17142857,\n",
       "        53.71428571, 54.25714286, 54.8       , 55.34285714, 55.88571429,\n",
       "        56.42857143, 56.97142857, 57.51428571, 58.05714286, 58.6       ,\n",
       "        59.14285714, 59.68571429, 60.22857143, 60.77142857, 61.31428571,\n",
       "        61.85714286, 62.4       , 62.94285714, 63.48571429, 64.02857143,\n",
       "        64.57142857, 65.11428571, 65.65714286, 66.2       , 66.74285714,\n",
       "        67.28571429, 67.82857143, 68.37142857, 68.91428571, 69.45714286,\n",
       "        70.        ]),\n",
       " <a list of 70 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['age'],bins=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.028979</td>\n",
       "      <td>0.017352</td>\n",
       "      <td>0.197596</td>\n",
       "      <td>0.317930</td>\n",
       "      <td>-0.052506</td>\n",
       "      <td>-0.004546</td>\n",
       "      <td>0.005313</td>\n",
       "      <td>0.015708</td>\n",
       "      <td>-0.070322</td>\n",
       "      <td>-0.035989</td>\n",
       "      <td>0.057933</td>\n",
       "      <td>0.081672</td>\n",
       "      <td>-0.116620</td>\n",
       "      <td>0.006083</td>\n",
       "      <td>0.088428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>-0.028979</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.165883</td>\n",
       "      <td>-0.213748</td>\n",
       "      <td>-0.192791</td>\n",
       "      <td>0.122995</td>\n",
       "      <td>0.057655</td>\n",
       "      <td>0.307194</td>\n",
       "      <td>0.101258</td>\n",
       "      <td>0.262131</td>\n",
       "      <td>0.394302</td>\n",
       "      <td>0.206104</td>\n",
       "      <td>0.135800</td>\n",
       "      <td>-0.012823</td>\n",
       "      <td>0.122256</td>\n",
       "      <td>0.225256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>0.017352</td>\n",
       "      <td>-0.165883</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018532</td>\n",
       "      <td>0.008085</td>\n",
       "      <td>-0.010815</td>\n",
       "      <td>-0.035112</td>\n",
       "      <td>-0.081970</td>\n",
       "      <td>-0.038680</td>\n",
       "      <td>-0.023115</td>\n",
       "      <td>-0.129631</td>\n",
       "      <td>-0.062316</td>\n",
       "      <td>-0.137504</td>\n",
       "      <td>-0.054206</td>\n",
       "      <td>-0.035721</td>\n",
       "      <td>-0.054059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currentSmoker</th>\n",
       "      <td>0.197596</td>\n",
       "      <td>-0.213748</td>\n",
       "      <td>0.018532</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769690</td>\n",
       "      <td>-0.048938</td>\n",
       "      <td>-0.032988</td>\n",
       "      <td>-0.103260</td>\n",
       "      <td>-0.044295</td>\n",
       "      <td>-0.046562</td>\n",
       "      <td>-0.130230</td>\n",
       "      <td>-0.107746</td>\n",
       "      <td>-0.167650</td>\n",
       "      <td>0.062356</td>\n",
       "      <td>-0.056826</td>\n",
       "      <td>0.019456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cigsPerDay</th>\n",
       "      <td>0.317930</td>\n",
       "      <td>-0.192791</td>\n",
       "      <td>0.008085</td>\n",
       "      <td>0.769690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.046134</td>\n",
       "      <td>-0.032707</td>\n",
       "      <td>-0.066146</td>\n",
       "      <td>-0.037067</td>\n",
       "      <td>-0.026320</td>\n",
       "      <td>-0.088780</td>\n",
       "      <td>-0.056632</td>\n",
       "      <td>-0.092856</td>\n",
       "      <td>0.075157</td>\n",
       "      <td>-0.058960</td>\n",
       "      <td>0.057884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BPMeds</th>\n",
       "      <td>-0.052506</td>\n",
       "      <td>0.122995</td>\n",
       "      <td>-0.010815</td>\n",
       "      <td>-0.048938</td>\n",
       "      <td>-0.046134</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.117365</td>\n",
       "      <td>0.261187</td>\n",
       "      <td>0.052047</td>\n",
       "      <td>0.080558</td>\n",
       "      <td>0.254219</td>\n",
       "      <td>0.194227</td>\n",
       "      <td>0.100668</td>\n",
       "      <td>0.015233</td>\n",
       "      <td>0.051176</td>\n",
       "      <td>0.087489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prevalentStroke</th>\n",
       "      <td>-0.004546</td>\n",
       "      <td>0.057655</td>\n",
       "      <td>-0.035112</td>\n",
       "      <td>-0.032988</td>\n",
       "      <td>-0.032707</td>\n",
       "      <td>0.117365</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.074830</td>\n",
       "      <td>0.006949</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.057009</td>\n",
       "      <td>0.045190</td>\n",
       "      <td>0.025891</td>\n",
       "      <td>-0.017676</td>\n",
       "      <td>0.018431</td>\n",
       "      <td>0.061810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prevalentHyp</th>\n",
       "      <td>0.005313</td>\n",
       "      <td>0.307194</td>\n",
       "      <td>-0.081970</td>\n",
       "      <td>-0.103260</td>\n",
       "      <td>-0.066146</td>\n",
       "      <td>0.261187</td>\n",
       "      <td>0.074830</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077808</td>\n",
       "      <td>0.163993</td>\n",
       "      <td>0.696755</td>\n",
       "      <td>0.615751</td>\n",
       "      <td>0.301318</td>\n",
       "      <td>0.147261</td>\n",
       "      <td>0.086834</td>\n",
       "      <td>0.177603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetes</th>\n",
       "      <td>0.015708</td>\n",
       "      <td>0.101258</td>\n",
       "      <td>-0.038680</td>\n",
       "      <td>-0.044295</td>\n",
       "      <td>-0.037067</td>\n",
       "      <td>0.052047</td>\n",
       "      <td>0.006949</td>\n",
       "      <td>0.077808</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.040278</td>\n",
       "      <td>0.111283</td>\n",
       "      <td>0.050329</td>\n",
       "      <td>0.087036</td>\n",
       "      <td>0.048994</td>\n",
       "      <td>0.617627</td>\n",
       "      <td>0.097317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totChol</th>\n",
       "      <td>-0.070322</td>\n",
       "      <td>0.262131</td>\n",
       "      <td>-0.023115</td>\n",
       "      <td>-0.046562</td>\n",
       "      <td>-0.026320</td>\n",
       "      <td>0.080558</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.163993</td>\n",
       "      <td>0.040278</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.208908</td>\n",
       "      <td>0.165182</td>\n",
       "      <td>0.115767</td>\n",
       "      <td>0.091125</td>\n",
       "      <td>0.046408</td>\n",
       "      <td>0.082184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sysBP</th>\n",
       "      <td>-0.035989</td>\n",
       "      <td>0.394302</td>\n",
       "      <td>-0.129631</td>\n",
       "      <td>-0.130230</td>\n",
       "      <td>-0.088780</td>\n",
       "      <td>0.254219</td>\n",
       "      <td>0.057009</td>\n",
       "      <td>0.696755</td>\n",
       "      <td>0.111283</td>\n",
       "      <td>0.208908</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.784002</td>\n",
       "      <td>0.326981</td>\n",
       "      <td>0.182246</td>\n",
       "      <td>0.140621</td>\n",
       "      <td>0.216429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diaBP</th>\n",
       "      <td>0.057933</td>\n",
       "      <td>0.206104</td>\n",
       "      <td>-0.062316</td>\n",
       "      <td>-0.107746</td>\n",
       "      <td>-0.056632</td>\n",
       "      <td>0.194227</td>\n",
       "      <td>0.045190</td>\n",
       "      <td>0.615751</td>\n",
       "      <td>0.050329</td>\n",
       "      <td>0.165182</td>\n",
       "      <td>0.784002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.377588</td>\n",
       "      <td>0.181255</td>\n",
       "      <td>0.061231</td>\n",
       "      <td>0.145299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>0.081672</td>\n",
       "      <td>0.135800</td>\n",
       "      <td>-0.137504</td>\n",
       "      <td>-0.167650</td>\n",
       "      <td>-0.092856</td>\n",
       "      <td>0.100668</td>\n",
       "      <td>0.025891</td>\n",
       "      <td>0.301318</td>\n",
       "      <td>0.087036</td>\n",
       "      <td>0.115767</td>\n",
       "      <td>0.326981</td>\n",
       "      <td>0.377588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.067678</td>\n",
       "      <td>0.087377</td>\n",
       "      <td>0.075192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heartRate</th>\n",
       "      <td>-0.116620</td>\n",
       "      <td>-0.012823</td>\n",
       "      <td>-0.054206</td>\n",
       "      <td>0.062356</td>\n",
       "      <td>0.075157</td>\n",
       "      <td>0.015233</td>\n",
       "      <td>-0.017676</td>\n",
       "      <td>0.147261</td>\n",
       "      <td>0.048994</td>\n",
       "      <td>0.091125</td>\n",
       "      <td>0.182246</td>\n",
       "      <td>0.181255</td>\n",
       "      <td>0.067678</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.094500</td>\n",
       "      <td>0.022913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glucose</th>\n",
       "      <td>0.006083</td>\n",
       "      <td>0.122256</td>\n",
       "      <td>-0.035721</td>\n",
       "      <td>-0.056826</td>\n",
       "      <td>-0.058960</td>\n",
       "      <td>0.051176</td>\n",
       "      <td>0.018431</td>\n",
       "      <td>0.086834</td>\n",
       "      <td>0.617627</td>\n",
       "      <td>0.046408</td>\n",
       "      <td>0.140621</td>\n",
       "      <td>0.061231</td>\n",
       "      <td>0.087377</td>\n",
       "      <td>0.094500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TenYearCHD</th>\n",
       "      <td>0.088428</td>\n",
       "      <td>0.225256</td>\n",
       "      <td>-0.054059</td>\n",
       "      <td>0.019456</td>\n",
       "      <td>0.057884</td>\n",
       "      <td>0.087489</td>\n",
       "      <td>0.061810</td>\n",
       "      <td>0.177603</td>\n",
       "      <td>0.097317</td>\n",
       "      <td>0.082184</td>\n",
       "      <td>0.216429</td>\n",
       "      <td>0.145299</td>\n",
       "      <td>0.075192</td>\n",
       "      <td>0.022913</td>\n",
       "      <td>0.125544</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   gender       age  education  currentSmoker  cigsPerDay  \\\n",
       "gender           1.000000 -0.028979   0.017352       0.197596    0.317930   \n",
       "age             -0.028979  1.000000  -0.165883      -0.213748   -0.192791   \n",
       "education        0.017352 -0.165883   1.000000       0.018532    0.008085   \n",
       "currentSmoker    0.197596 -0.213748   0.018532       1.000000    0.769690   \n",
       "cigsPerDay       0.317930 -0.192791   0.008085       0.769690    1.000000   \n",
       "BPMeds          -0.052506  0.122995  -0.010815      -0.048938   -0.046134   \n",
       "prevalentStroke -0.004546  0.057655  -0.035112      -0.032988   -0.032707   \n",
       "prevalentHyp     0.005313  0.307194  -0.081970      -0.103260   -0.066146   \n",
       "diabetes         0.015708  0.101258  -0.038680      -0.044295   -0.037067   \n",
       "totChol         -0.070322  0.262131  -0.023115      -0.046562   -0.026320   \n",
       "sysBP           -0.035989  0.394302  -0.129631      -0.130230   -0.088780   \n",
       "diaBP            0.057933  0.206104  -0.062316      -0.107746   -0.056632   \n",
       "BMI              0.081672  0.135800  -0.137504      -0.167650   -0.092856   \n",
       "heartRate       -0.116620 -0.012823  -0.054206       0.062356    0.075157   \n",
       "glucose          0.006083  0.122256  -0.035721      -0.056826   -0.058960   \n",
       "TenYearCHD       0.088428  0.225256  -0.054059       0.019456    0.057884   \n",
       "\n",
       "                   BPMeds  prevalentStroke  prevalentHyp  diabetes   totChol  \\\n",
       "gender          -0.052506        -0.004546      0.005313  0.015708 -0.070322   \n",
       "age              0.122995         0.057655      0.307194  0.101258  0.262131   \n",
       "education       -0.010815        -0.035112     -0.081970 -0.038680 -0.023115   \n",
       "currentSmoker   -0.048938        -0.032988     -0.103260 -0.044295 -0.046562   \n",
       "cigsPerDay      -0.046134        -0.032707     -0.066146 -0.037067 -0.026320   \n",
       "BPMeds           1.000000         0.117365      0.261187  0.052047  0.080558   \n",
       "prevalentStroke  0.117365         1.000000      0.074830  0.006949  0.000067   \n",
       "prevalentHyp     0.261187         0.074830      1.000000  0.077808  0.163993   \n",
       "diabetes         0.052047         0.006949      0.077808  1.000000  0.040278   \n",
       "totChol          0.080558         0.000067      0.163993  0.040278  1.000000   \n",
       "sysBP            0.254219         0.057009      0.696755  0.111283  0.208908   \n",
       "diaBP            0.194227         0.045190      0.615751  0.050329  0.165182   \n",
       "BMI              0.100668         0.025891      0.301318  0.087036  0.115767   \n",
       "heartRate        0.015233        -0.017676      0.147261  0.048994  0.091125   \n",
       "glucose          0.051176         0.018431      0.086834  0.617627  0.046408   \n",
       "TenYearCHD       0.087489         0.061810      0.177603  0.097317  0.082184   \n",
       "\n",
       "                    sysBP     diaBP       BMI  heartRate   glucose  TenYearCHD  \n",
       "gender          -0.035989  0.057933  0.081672  -0.116620  0.006083    0.088428  \n",
       "age              0.394302  0.206104  0.135800  -0.012823  0.122256    0.225256  \n",
       "education       -0.129631 -0.062316 -0.137504  -0.054206 -0.035721   -0.054059  \n",
       "currentSmoker   -0.130230 -0.107746 -0.167650   0.062356 -0.056826    0.019456  \n",
       "cigsPerDay      -0.088780 -0.056632 -0.092856   0.075157 -0.058960    0.057884  \n",
       "BPMeds           0.254219  0.194227  0.100668   0.015233  0.051176    0.087489  \n",
       "prevalentStroke  0.057009  0.045190  0.025891  -0.017676  0.018431    0.061810  \n",
       "prevalentHyp     0.696755  0.615751  0.301318   0.147261  0.086834    0.177603  \n",
       "diabetes         0.111283  0.050329  0.087036   0.048994  0.617627    0.097317  \n",
       "totChol          0.208908  0.165182  0.115767   0.091125  0.046408    0.082184  \n",
       "sysBP            1.000000  0.784002  0.326981   0.182246  0.140621    0.216429  \n",
       "diaBP            0.784002  1.000000  0.377588   0.181255  0.061231    0.145299  \n",
       "BMI              0.326981  0.377588  1.000000   0.067678  0.087377    0.075192  \n",
       "heartRate        0.182246  0.181255  0.067678   1.000000  0.094500    0.022913  \n",
       "glucose          0.140621  0.061231  0.087377   0.094500  1.000000    0.125544  \n",
       "TenYearCHD       0.216429  0.145299  0.075192   0.022913  0.125544    1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('education', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(\"TenYearCHD\",axis=1).values\n",
    "y = df[\"TenYearCHD\"].values\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30,random_state=101)\n",
    "\n",
    "\n",
    "#scaling data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=15,activation='relu'))#, kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(units=7,activation='relu'))#, kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(units=3,activation='relu'))#, kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2559 samples, validate on 1097 samples\n",
      "Epoch 1/600\n",
      "2559/2559 [==============================] - 1s 440us/sample - loss: 0.6804 - val_loss: 0.6657\n",
      "Epoch 2/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.6020 - val_loss: 0.4995\n",
      "Epoch 3/600\n",
      "2559/2559 [==============================] - 0s 134us/sample - loss: 0.4668 - val_loss: 0.4295\n",
      "Epoch 4/600\n",
      "2559/2559 [==============================] - 0s 113us/sample - loss: 0.4485 - val_loss: 0.4185\n",
      "Epoch 5/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.4396 - val_loss: 0.4080\n",
      "Epoch 6/600\n",
      "2559/2559 [==============================] - 0s 139us/sample - loss: 0.4300 - val_loss: 0.3966\n",
      "Epoch 7/600\n",
      "2559/2559 [==============================] - 0s 99us/sample - loss: 0.4237 - val_loss: 0.3906\n",
      "Epoch 8/600\n",
      "2559/2559 [==============================] - 0s 90us/sample - loss: 0.4184 - val_loss: 0.3847\n",
      "Epoch 9/600\n",
      "2559/2559 [==============================] - 0s 124us/sample - loss: 0.4157 - val_loss: 0.3835\n",
      "Epoch 10/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.4126 - val_loss: 0.3823\n",
      "Epoch 11/600\n",
      "2559/2559 [==============================] - 0s 81us/sample - loss: 0.4107 - val_loss: 0.3774\n",
      "Epoch 12/600\n",
      "2559/2559 [==============================] - 0s 83us/sample - loss: 0.4087 - val_loss: 0.3779\n",
      "Epoch 13/600\n",
      "2559/2559 [==============================] - 0s 81us/sample - loss: 0.4063 - val_loss: 0.3719\n",
      "Epoch 14/600\n",
      "2559/2559 [==============================] - 0s 80us/sample - loss: 0.4060 - val_loss: 0.3736\n",
      "Epoch 15/600\n",
      "2559/2559 [==============================] - 0s 81us/sample - loss: 0.4032 - val_loss: 0.3714\n",
      "Epoch 16/600\n",
      "2559/2559 [==============================] - 0s 86us/sample - loss: 0.4034 - val_loss: 0.3672\n",
      "Epoch 17/600\n",
      "2559/2559 [==============================] - 0s 85us/sample - loss: 0.4015 - val_loss: 0.3666\n",
      "Epoch 18/600\n",
      "2559/2559 [==============================] - 0s 78us/sample - loss: 0.3993 - val_loss: 0.3649\n",
      "Epoch 19/600\n",
      "2559/2559 [==============================] - 0s 128us/sample - loss: 0.3991 - val_loss: 0.3650\n",
      "Epoch 20/600\n",
      "2559/2559 [==============================] - 0s 84us/sample - loss: 0.3983 - val_loss: 0.3647\n",
      "Epoch 21/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.3972 - val_loss: 0.3645\n",
      "Epoch 22/600\n",
      "2559/2559 [==============================] - 0s 79us/sample - loss: 0.3973 - val_loss: 0.3651\n",
      "Epoch 23/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.3964 - val_loss: 0.3622\n",
      "Epoch 24/600\n",
      "2559/2559 [==============================] - 0s 77us/sample - loss: 0.3948 - val_loss: 0.3657\n",
      "Epoch 25/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3935 - val_loss: 0.3614\n",
      "Epoch 26/600\n",
      "2559/2559 [==============================] - 0s 81us/sample - loss: 0.3942 - val_loss: 0.3629\n",
      "Epoch 27/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3940 - val_loss: 0.3600\n",
      "Epoch 28/600\n",
      "2559/2559 [==============================] - 0s 82us/sample - loss: 0.3941 - val_loss: 0.3604\n",
      "Epoch 29/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3927 - val_loss: 0.3619\n",
      "Epoch 30/600\n",
      "2559/2559 [==============================] - 0s 78us/sample - loss: 0.3943 - val_loss: 0.3592\n",
      "Epoch 31/600\n",
      "2559/2559 [==============================] - 0s 69us/sample - loss: 0.3928 - val_loss: 0.3613\n",
      "Epoch 32/600\n",
      "2559/2559 [==============================] - 0s 81us/sample - loss: 0.3923 - val_loss: 0.3598\n",
      "Epoch 33/600\n",
      "2559/2559 [==============================] - 0s 81us/sample - loss: 0.3911 - val_loss: 0.3590\n",
      "Epoch 34/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3918 - val_loss: 0.3594\n",
      "Epoch 35/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3916 - val_loss: 0.3600\n",
      "Epoch 36/600\n",
      "2559/2559 [==============================] - 0s 79us/sample - loss: 0.3901 - val_loss: 0.3594\n",
      "Epoch 37/600\n",
      "2559/2559 [==============================] - 0s 83us/sample - loss: 0.3892 - val_loss: 0.3604\n",
      "Epoch 38/600\n",
      "2559/2559 [==============================] - 0s 143us/sample - loss: 0.3899 - val_loss: 0.3622\n",
      "Epoch 39/600\n",
      "2559/2559 [==============================] - 0s 120us/sample - loss: 0.3901 - val_loss: 0.3593\n",
      "Epoch 40/600\n",
      "2559/2559 [==============================] - 0s 123us/sample - loss: 0.3887 - val_loss: 0.3611\n",
      "Epoch 41/600\n",
      "2559/2559 [==============================] - 0s 88us/sample - loss: 0.3886 - val_loss: 0.3580\n",
      "Epoch 42/600\n",
      "2559/2559 [==============================] - 0s 84us/sample - loss: 0.3886 - val_loss: 0.3572\n",
      "Epoch 43/600\n",
      "2559/2559 [==============================] - 0s 95us/sample - loss: 0.3879 - val_loss: 0.3592\n",
      "Epoch 44/600\n",
      "2559/2559 [==============================] - 0s 78us/sample - loss: 0.3887 - val_loss: 0.3572\n",
      "Epoch 45/600\n",
      "2559/2559 [==============================] - 0s 78us/sample - loss: 0.3884 - val_loss: 0.3579\n",
      "Epoch 46/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.3874 - val_loss: 0.3582\n",
      "Epoch 47/600\n",
      "2559/2559 [==============================] - 0s 81us/sample - loss: 0.3868 - val_loss: 0.3628\n",
      "Epoch 48/600\n",
      "2559/2559 [==============================] - 0s 91us/sample - loss: 0.3886 - val_loss: 0.3594\n",
      "Epoch 49/600\n",
      "2559/2559 [==============================] - 0s 80us/sample - loss: 0.3873 - val_loss: 0.3589\n",
      "Epoch 50/600\n",
      "2559/2559 [==============================] - 0s 91us/sample - loss: 0.3874 - val_loss: 0.3600\n",
      "Epoch 51/600\n",
      "2559/2559 [==============================] - 0s 79us/sample - loss: 0.3868 - val_loss: 0.3582\n",
      "Epoch 52/600\n",
      "2559/2559 [==============================] - 0s 84us/sample - loss: 0.3863 - val_loss: 0.3571\n",
      "Epoch 53/600\n",
      "2559/2559 [==============================] - 0s 134us/sample - loss: 0.3871 - val_loss: 0.3579\n",
      "Epoch 54/600\n",
      "2559/2559 [==============================] - 0s 139us/sample - loss: 0.3853 - val_loss: 0.3649\n",
      "Epoch 55/600\n",
      "2559/2559 [==============================] - 0s 111us/sample - loss: 0.3875 - val_loss: 0.3572\n",
      "Epoch 56/600\n",
      "2559/2559 [==============================] - 0s 128us/sample - loss: 0.3854 - val_loss: 0.3583\n",
      "Epoch 57/600\n",
      "2559/2559 [==============================] - 0s 83us/sample - loss: 0.3867 - val_loss: 0.3587\n",
      "Epoch 58/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.3865 - val_loss: 0.3573\n",
      "Epoch 59/600\n",
      "2559/2559 [==============================] - 0s 113us/sample - loss: 0.3858 - val_loss: 0.3585\n",
      "Epoch 60/600\n",
      "2559/2559 [==============================] - 0s 106us/sample - loss: 0.3859 - val_loss: 0.3575\n",
      "Epoch 61/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.3854 - val_loss: 0.3590\n",
      "Epoch 62/600\n",
      "2559/2559 [==============================] - 0s 63us/sample - loss: 0.3848 - val_loss: 0.3580\n",
      "Epoch 63/600\n",
      "2559/2559 [==============================] - 0s 92us/sample - loss: 0.3846 - val_loss: 0.3604\n",
      "Epoch 64/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.3858 - val_loss: 0.3567\n",
      "Epoch 65/600\n",
      "2559/2559 [==============================] - 0s 69us/sample - loss: 0.3847 - val_loss: 0.3587\n",
      "Epoch 66/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3854 - val_loss: 0.3592\n",
      "Epoch 67/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3853 - val_loss: 0.3582\n",
      "Epoch 68/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3842 - val_loss: 0.3611\n",
      "Epoch 69/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3838 - val_loss: 0.3584\n",
      "Epoch 70/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3841 - val_loss: 0.3593\n",
      "Epoch 71/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3838 - val_loss: 0.3574\n",
      "Epoch 72/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3827 - val_loss: 0.3582\n",
      "Epoch 73/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3839 - val_loss: 0.3626\n",
      "Epoch 74/600\n",
      "2559/2559 [==============================] - 0s 69us/sample - loss: 0.3840 - val_loss: 0.3578\n",
      "Epoch 75/600\n",
      "2559/2559 [==============================] - 0s 69us/sample - loss: 0.3836 - val_loss: 0.3591\n",
      "Epoch 76/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3832 - val_loss: 0.3587\n",
      "Epoch 77/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3830 - val_loss: 0.3582\n",
      "Epoch 78/600\n",
      "2559/2559 [==============================] - 0s 62us/sample - loss: 0.3826 - val_loss: 0.3576\n",
      "Epoch 79/600\n",
      "2559/2559 [==============================] - 0s 61us/sample - loss: 0.3825 - val_loss: 0.3599\n",
      "Epoch 80/600\n",
      "2559/2559 [==============================] - 0s 58us/sample - loss: 0.3828 - val_loss: 0.3678\n",
      "Epoch 81/600\n",
      "2559/2559 [==============================] - 0s 62us/sample - loss: 0.3846 - val_loss: 0.3607\n",
      "Epoch 82/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3837 - val_loss: 0.3575\n",
      "Epoch 83/600\n",
      "2559/2559 [==============================] - 0s 62us/sample - loss: 0.3821 - val_loss: 0.3580\n",
      "Epoch 84/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3830 - val_loss: 0.3591\n",
      "Epoch 85/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3814 - val_loss: 0.3588\n",
      "Epoch 86/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3829 - val_loss: 0.3605\n",
      "Epoch 87/600\n",
      "2559/2559 [==============================] - 0s 61us/sample - loss: 0.3820 - val_loss: 0.3668\n",
      "Epoch 88/600\n",
      "2559/2559 [==============================] - 0s 60us/sample - loss: 0.3835 - val_loss: 0.3611\n",
      "Epoch 89/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3829 - val_loss: 0.3625\n",
      "Epoch 90/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3827 - val_loss: 0.3589\n",
      "Epoch 91/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3815 - val_loss: 0.3593\n",
      "Epoch 92/600\n",
      "2559/2559 [==============================] - 0s 63us/sample - loss: 0.3816 - val_loss: 0.3578\n",
      "Epoch 93/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3817 - val_loss: 0.3578\n",
      "Epoch 94/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3819 - val_loss: 0.3597\n",
      "Epoch 95/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3816 - val_loss: 0.3595\n",
      "Epoch 96/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3819 - val_loss: 0.3587\n",
      "Epoch 97/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3815 - val_loss: 0.3597\n",
      "Epoch 98/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3810 - val_loss: 0.3639\n",
      "Epoch 99/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3826 - val_loss: 0.3605\n",
      "Epoch 100/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3812 - val_loss: 0.3598\n",
      "Epoch 101/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3802 - val_loss: 0.3598\n",
      "Epoch 102/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3808 - val_loss: 0.3587\n",
      "Epoch 103/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3809 - val_loss: 0.3577\n",
      "Epoch 104/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3808 - val_loss: 0.3597\n",
      "Epoch 105/600\n",
      "2559/2559 [==============================] - 0s 61us/sample - loss: 0.3812 - val_loss: 0.3592\n",
      "Epoch 106/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3794 - val_loss: 0.3590\n",
      "Epoch 107/600\n",
      "2559/2559 [==============================] - 0s 84us/sample - loss: 0.3813 - val_loss: 0.3591\n",
      "Epoch 108/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.3801 - val_loss: 0.3587\n",
      "Epoch 109/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.3802 - val_loss: 0.3598\n",
      "Epoch 110/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3802 - val_loss: 0.3581\n",
      "Epoch 111/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3791 - val_loss: 0.3604\n",
      "Epoch 112/600\n",
      "2559/2559 [==============================] - 0s 78us/sample - loss: 0.3800 - val_loss: 0.3614\n",
      "Epoch 113/600\n",
      "2559/2559 [==============================] - 0s 89us/sample - loss: 0.3799 - val_loss: 0.3593\n",
      "Epoch 114/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3803 - val_loss: 0.3623\n",
      "Epoch 115/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3806 - val_loss: 0.3578\n",
      "Epoch 116/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3795 - val_loss: 0.3590\n",
      "Epoch 117/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3802 - val_loss: 0.3590\n",
      "Epoch 118/600\n",
      "2559/2559 [==============================] - 0s 83us/sample - loss: 0.3791 - val_loss: 0.3581\n",
      "Epoch 119/600\n",
      "2559/2559 [==============================] - 0s 89us/sample - loss: 0.3807 - val_loss: 0.3581\n",
      "Epoch 120/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3792 - val_loss: 0.3589\n",
      "Epoch 121/600\n",
      "2559/2559 [==============================] - 0s 85us/sample - loss: 0.3810 - val_loss: 0.3620\n",
      "Epoch 122/600\n",
      "2559/2559 [==============================] - 0s 82us/sample - loss: 0.3797 - val_loss: 0.3607\n",
      "Epoch 123/600\n",
      "2559/2559 [==============================] - 0s 176us/sample - loss: 0.3802 - val_loss: 0.3577\n",
      "Epoch 124/600\n",
      "2559/2559 [==============================] - 0s 98us/sample - loss: 0.3787 - val_loss: 0.3579\n",
      "Epoch 125/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.3802 - val_loss: 0.3576\n",
      "Epoch 126/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3791 - val_loss: 0.3650\n",
      "Epoch 127/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.3800 - val_loss: 0.3591\n",
      "Epoch 128/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3805 - val_loss: 0.3603\n",
      "Epoch 129/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3812 - val_loss: 0.3593\n",
      "Epoch 130/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3792 - val_loss: 0.3573\n",
      "Epoch 131/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3791 - val_loss: 0.3608\n",
      "Epoch 132/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3799 - val_loss: 0.3606\n",
      "Epoch 133/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3790 - val_loss: 0.3585\n",
      "Epoch 134/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3783 - val_loss: 0.3587\n",
      "Epoch 135/600\n",
      "2559/2559 [==============================] - 0s 77us/sample - loss: 0.3793 - val_loss: 0.3598\n",
      "Epoch 136/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3784 - val_loss: 0.3608\n",
      "Epoch 137/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3790 - val_loss: 0.3588\n",
      "Epoch 138/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3789 - val_loss: 0.3621\n",
      "Epoch 139/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3786 - val_loss: 0.3583\n",
      "Epoch 140/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3800 - val_loss: 0.3590\n",
      "Epoch 141/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3784 - val_loss: 0.3601\n",
      "Epoch 142/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3785 - val_loss: 0.3608\n",
      "Epoch 143/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3791 - val_loss: 0.3605\n",
      "Epoch 144/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3789 - val_loss: 0.3590\n",
      "Epoch 145/600\n",
      "2559/2559 [==============================] - 0s 78us/sample - loss: 0.3782 - val_loss: 0.3597\n",
      "Epoch 146/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3784 - val_loss: 0.3584\n",
      "Epoch 147/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3782 - val_loss: 0.3617\n",
      "Epoch 148/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3791 - val_loss: 0.3606\n",
      "Epoch 149/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3797 - val_loss: 0.3588\n",
      "Epoch 150/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.3785 - val_loss: 0.3581\n",
      "Epoch 151/600\n",
      "2559/2559 [==============================] - 0s 78us/sample - loss: 0.3783 - val_loss: 0.3582\n",
      "Epoch 152/600\n",
      "2559/2559 [==============================] - 0s 63us/sample - loss: 0.3787 - val_loss: 0.3586\n",
      "Epoch 153/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3780 - val_loss: 0.3611\n",
      "Epoch 154/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3788 - val_loss: 0.3596\n",
      "Epoch 155/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3786 - val_loss: 0.3591\n",
      "Epoch 156/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3785 - val_loss: 0.3598\n",
      "Epoch 157/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3778 - val_loss: 0.3610\n",
      "Epoch 158/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3775 - val_loss: 0.3596\n",
      "Epoch 159/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3781 - val_loss: 0.3602\n",
      "Epoch 160/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3774 - val_loss: 0.3612\n",
      "Epoch 161/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3784 - val_loss: 0.3599\n",
      "Epoch 162/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.3775 - val_loss: 0.3598\n",
      "Epoch 163/600\n",
      "2559/2559 [==============================] - 0s 63us/sample - loss: 0.3782 - val_loss: 0.3609\n",
      "Epoch 164/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3785 - val_loss: 0.3587\n",
      "Epoch 165/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3775 - val_loss: 0.3585\n",
      "Epoch 166/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3773 - val_loss: 0.3591\n",
      "Epoch 167/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3803 - val_loss: 0.3636\n",
      "Epoch 168/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3772 - val_loss: 0.3617\n",
      "Epoch 169/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3775 - val_loss: 0.3589\n",
      "Epoch 170/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3776 - val_loss: 0.3594\n",
      "Epoch 171/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3782 - val_loss: 0.3605\n",
      "Epoch 172/600\n",
      "2559/2559 [==============================] - 0s 60us/sample - loss: 0.3770 - val_loss: 0.3588\n",
      "Epoch 173/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3779 - val_loss: 0.3637\n",
      "Epoch 174/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3780 - val_loss: 0.3588\n",
      "Epoch 175/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3774 - val_loss: 0.3585\n",
      "Epoch 176/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3774 - val_loss: 0.3604\n",
      "Epoch 177/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3765 - val_loss: 0.3654\n",
      "Epoch 178/600\n",
      "2559/2559 [==============================] - 0s 77us/sample - loss: 0.3782 - val_loss: 0.3607\n",
      "Epoch 179/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3761 - val_loss: 0.3663\n",
      "Epoch 180/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3774 - val_loss: 0.3602\n",
      "Epoch 181/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3766 - val_loss: 0.3606\n",
      "Epoch 182/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3765 - val_loss: 0.3597\n",
      "Epoch 183/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3763 - val_loss: 0.3602\n",
      "Epoch 184/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3775 - val_loss: 0.3609\n",
      "Epoch 185/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3760 - val_loss: 0.3597\n",
      "Epoch 186/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3778 - val_loss: 0.3630\n",
      "Epoch 187/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3771 - val_loss: 0.3601\n",
      "Epoch 188/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3772 - val_loss: 0.3600\n",
      "Epoch 189/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3765 - val_loss: 0.3597\n",
      "Epoch 190/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3779 - val_loss: 0.3610\n",
      "Epoch 191/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3776 - val_loss: 0.3609\n",
      "Epoch 192/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.3769 - val_loss: 0.3626\n",
      "Epoch 193/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3768 - val_loss: 0.3609\n",
      "Epoch 194/600\n",
      "2559/2559 [==============================] - 0s 83us/sample - loss: 0.3767 - val_loss: 0.3607\n",
      "Epoch 195/600\n",
      "2559/2559 [==============================] - 0s 77us/sample - loss: 0.3756 - val_loss: 0.3618\n",
      "Epoch 196/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3762 - val_loss: 0.3602\n",
      "Epoch 197/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3762 - val_loss: 0.3606\n",
      "Epoch 198/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3762 - val_loss: 0.3614\n",
      "Epoch 199/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3756 - val_loss: 0.3631\n",
      "Epoch 200/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3770 - val_loss: 0.3596\n",
      "Epoch 201/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3765 - val_loss: 0.3600\n",
      "Epoch 202/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3764 - val_loss: 0.3636\n",
      "Epoch 203/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3758 - val_loss: 0.3609\n",
      "Epoch 204/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3748 - val_loss: 0.3598\n",
      "Epoch 205/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3766 - val_loss: 0.3613\n",
      "Epoch 206/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3757 - val_loss: 0.3601\n",
      "Epoch 207/600\n",
      "2559/2559 [==============================] - 0s 80us/sample - loss: 0.3756 - val_loss: 0.3619\n",
      "Epoch 208/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3760 - val_loss: 0.3614\n",
      "Epoch 209/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3756 - val_loss: 0.3610\n",
      "Epoch 210/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3761 - val_loss: 0.3606\n",
      "Epoch 211/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3750 - val_loss: 0.3611\n",
      "Epoch 212/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3761 - val_loss: 0.3610\n",
      "Epoch 213/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3754 - val_loss: 0.3632\n",
      "Epoch 214/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3757 - val_loss: 0.3600\n",
      "Epoch 215/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3757 - val_loss: 0.3603\n",
      "Epoch 216/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3747 - val_loss: 0.3610\n",
      "Epoch 217/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3747 - val_loss: 0.3617\n",
      "Epoch 218/600\n",
      "2559/2559 [==============================] - 0s 69us/sample - loss: 0.3753 - val_loss: 0.3628\n",
      "Epoch 219/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3754 - val_loss: 0.3632\n",
      "Epoch 220/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3747 - val_loss: 0.3618\n",
      "Epoch 221/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3746 - val_loss: 0.3654\n",
      "Epoch 222/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3758 - val_loss: 0.3630\n",
      "Epoch 223/600\n",
      "2559/2559 [==============================] - 0s 69us/sample - loss: 0.3748 - val_loss: 0.3628\n",
      "Epoch 224/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3731 - val_loss: 0.3623\n",
      "Epoch 225/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3753 - val_loss: 0.3609\n",
      "Epoch 226/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3755 - val_loss: 0.3639\n",
      "Epoch 227/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3748 - val_loss: 0.3611\n",
      "Epoch 228/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3750 - val_loss: 0.3602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3740 - val_loss: 0.3629\n",
      "Epoch 230/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3751 - val_loss: 0.3621\n",
      "Epoch 231/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3747 - val_loss: 0.3610\n",
      "Epoch 232/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3738 - val_loss: 0.3633\n",
      "Epoch 233/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3740 - val_loss: 0.3612\n",
      "Epoch 234/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3745 - val_loss: 0.3607\n",
      "Epoch 235/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3746 - val_loss: 0.3625\n",
      "Epoch 236/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3752 - val_loss: 0.3614\n",
      "Epoch 237/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3751 - val_loss: 0.3656\n",
      "Epoch 238/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3738 - val_loss: 0.3629\n",
      "Epoch 239/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3734 - val_loss: 0.3623\n",
      "Epoch 240/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3741 - val_loss: 0.3628\n",
      "Epoch 241/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3757 - val_loss: 0.3628\n",
      "Epoch 242/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3729 - val_loss: 0.3622\n",
      "Epoch 243/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3743 - val_loss: 0.3637\n",
      "Epoch 244/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3733 - val_loss: 0.3625\n",
      "Epoch 245/600\n",
      "2559/2559 [==============================] - 0s 93us/sample - loss: 0.3739 - val_loss: 0.3656\n",
      "Epoch 246/600\n",
      "2559/2559 [==============================] - 0s 143us/sample - loss: 0.3741 - val_loss: 0.3632\n",
      "Epoch 247/600\n",
      "2559/2559 [==============================] - 0s 78us/sample - loss: 0.3726 - val_loss: 0.3624\n",
      "Epoch 248/600\n",
      "2559/2559 [==============================] - 0s 82us/sample - loss: 0.3730 - val_loss: 0.3643\n",
      "Epoch 249/600\n",
      "2559/2559 [==============================] - 0s 79us/sample - loss: 0.3745 - val_loss: 0.3646\n",
      "Epoch 250/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3721 - val_loss: 0.3659\n",
      "Epoch 251/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3756 - val_loss: 0.3622\n",
      "Epoch 252/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3740 - val_loss: 0.3626\n",
      "Epoch 253/600\n",
      "2559/2559 [==============================] - 0s 62us/sample - loss: 0.3729 - val_loss: 0.3614\n",
      "Epoch 254/600\n",
      "2559/2559 [==============================] - 0s 101us/sample - loss: 0.3738 - val_loss: 0.3612\n",
      "Epoch 255/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3739 - val_loss: 0.3616\n",
      "Epoch 256/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3729 - val_loss: 0.3638\n",
      "Epoch 257/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3748 - val_loss: 0.3625\n",
      "Epoch 258/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3728 - val_loss: 0.3627\n",
      "Epoch 259/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3731 - val_loss: 0.3637\n",
      "Epoch 260/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3724 - val_loss: 0.3628\n",
      "Epoch 261/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3741 - val_loss: 0.3625\n",
      "Epoch 262/600\n",
      "2559/2559 [==============================] - 0s 84us/sample - loss: 0.3732 - val_loss: 0.3611\n",
      "Epoch 263/600\n",
      "2559/2559 [==============================] - 0s 79us/sample - loss: 0.3734 - val_loss: 0.3619\n",
      "Epoch 264/600\n",
      "2559/2559 [==============================] - 0s 63us/sample - loss: 0.3743 - val_loss: 0.3628\n",
      "Epoch 265/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3726 - val_loss: 0.3637\n",
      "Epoch 266/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3734 - val_loss: 0.3625\n",
      "Epoch 267/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3723 - val_loss: 0.3630\n",
      "Epoch 268/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3725 - val_loss: 0.3638\n",
      "Epoch 269/600\n",
      "2559/2559 [==============================] - 0s 79us/sample - loss: 0.3739 - val_loss: 0.3633\n",
      "Epoch 270/600\n",
      "2559/2559 [==============================] - 0s 84us/sample - loss: 0.3735 - val_loss: 0.3641\n",
      "Epoch 271/600\n",
      "2559/2559 [==============================] - 0s 86us/sample - loss: 0.3741 - val_loss: 0.3633\n",
      "Epoch 272/600\n",
      "2559/2559 [==============================] - 0s 91us/sample - loss: 0.3726 - val_loss: 0.3662\n",
      "Epoch 273/600\n",
      "2559/2559 [==============================] - 0s 89us/sample - loss: 0.3729 - val_loss: 0.3628\n",
      "Epoch 274/600\n",
      "2559/2559 [==============================] - 0s 96us/sample - loss: 0.3735 - val_loss: 0.3626\n",
      "Epoch 275/600\n",
      "2559/2559 [==============================] - 0s 92us/sample - loss: 0.3731 - val_loss: 0.3623\n",
      "Epoch 276/600\n",
      "2559/2559 [==============================] - 0s 86us/sample - loss: 0.3725 - val_loss: 0.3656\n",
      "Epoch 277/600\n",
      "2559/2559 [==============================] - 0s 91us/sample - loss: 0.3733 - val_loss: 0.3647\n",
      "Epoch 278/600\n",
      "2559/2559 [==============================] - 0s 82us/sample - loss: 0.3724 - val_loss: 0.3644\n",
      "Epoch 279/600\n",
      "2559/2559 [==============================] - 0s 96us/sample - loss: 0.3740 - val_loss: 0.3656\n",
      "Epoch 280/600\n",
      "2559/2559 [==============================] - 0s 87us/sample - loss: 0.3733 - val_loss: 0.3626\n",
      "Epoch 281/600\n",
      "2559/2559 [==============================] - 0s 88us/sample - loss: 0.3725 - val_loss: 0.3639\n",
      "Epoch 282/600\n",
      "2559/2559 [==============================] - 0s 84us/sample - loss: 0.3723 - val_loss: 0.3638\n",
      "Epoch 283/600\n",
      "2559/2559 [==============================] - 0s 105us/sample - loss: 0.3730 - val_loss: 0.3627\n",
      "Epoch 284/600\n",
      "2559/2559 [==============================] - 0s 131us/sample - loss: 0.3738 - val_loss: 0.3714\n",
      "Epoch 285/600\n",
      "2559/2559 [==============================] - 0s 82us/sample - loss: 0.3724 - val_loss: 0.3637\n",
      "Epoch 286/600\n",
      "2559/2559 [==============================] - 0s 79us/sample - loss: 0.3727 - val_loss: 0.3680\n",
      "Epoch 287/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3733 - val_loss: 0.3672\n",
      "Epoch 288/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.3712 - val_loss: 0.3646\n",
      "Epoch 289/600\n",
      "2559/2559 [==============================] - 0s 79us/sample - loss: 0.3741 - val_loss: 0.3622\n",
      "Epoch 290/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3732 - val_loss: 0.3641\n",
      "Epoch 291/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3722 - val_loss: 0.3635\n",
      "Epoch 292/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3722 - val_loss: 0.3638\n",
      "Epoch 293/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3716 - val_loss: 0.3699\n",
      "Epoch 294/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3732 - val_loss: 0.3635\n",
      "Epoch 295/600\n",
      "2559/2559 [==============================] - 0s 87us/sample - loss: 0.3718 - val_loss: 0.3739\n",
      "Epoch 296/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3738 - val_loss: 0.3661\n",
      "Epoch 297/600\n",
      "2559/2559 [==============================] - 0s 83us/sample - loss: 0.3730 - val_loss: 0.3648\n",
      "Epoch 298/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3722 - val_loss: 0.3630\n",
      "Epoch 299/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3734 - val_loss: 0.3642\n",
      "Epoch 300/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3742 - val_loss: 0.3625\n",
      "Epoch 301/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3723 - val_loss: 0.3639\n",
      "Epoch 302/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3731 - val_loss: 0.3712\n",
      "Epoch 303/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3730 - val_loss: 0.3615\n",
      "Epoch 304/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3728 - val_loss: 0.3650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 305/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3736 - val_loss: 0.3623\n",
      "Epoch 306/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3722 - val_loss: 0.3629\n",
      "Epoch 307/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3721 - val_loss: 0.3654\n",
      "Epoch 308/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3717 - val_loss: 0.3647\n",
      "Epoch 309/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3727 - val_loss: 0.3643\n",
      "Epoch 310/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3721 - val_loss: 0.3657\n",
      "Epoch 311/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3715 - val_loss: 0.3635\n",
      "Epoch 312/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3719 - val_loss: 0.3657\n",
      "Epoch 313/600\n",
      "2559/2559 [==============================] - 0s 69us/sample - loss: 0.3731 - val_loss: 0.3640\n",
      "Epoch 314/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3734 - val_loss: 0.3638\n",
      "Epoch 315/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3727 - val_loss: 0.3631\n",
      "Epoch 316/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3712 - val_loss: 0.3635\n",
      "Epoch 317/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3710 - val_loss: 0.3639\n",
      "Epoch 318/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3720 - val_loss: 0.3643\n",
      "Epoch 319/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3727 - val_loss: 0.3648\n",
      "Epoch 320/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3715 - val_loss: 0.3636\n",
      "Epoch 321/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3710 - val_loss: 0.3660\n",
      "Epoch 322/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3713 - val_loss: 0.3628\n",
      "Epoch 323/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.3715 - val_loss: 0.3647\n",
      "Epoch 324/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3710 - val_loss: 0.3676\n",
      "Epoch 325/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3716 - val_loss: 0.3626\n",
      "Epoch 326/600\n",
      "2559/2559 [==============================] - 0s 69us/sample - loss: 0.3719 - val_loss: 0.3660\n",
      "Epoch 327/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3715 - val_loss: 0.3625\n",
      "Epoch 328/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3708 - val_loss: 0.3667\n",
      "Epoch 329/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3704 - val_loss: 0.3675\n",
      "Epoch 330/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3733 - val_loss: 0.3664\n",
      "Epoch 331/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3712 - val_loss: 0.3652\n",
      "Epoch 332/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3719 - val_loss: 0.3620\n",
      "Epoch 333/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3728 - val_loss: 0.3653\n",
      "Epoch 334/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.3704 - val_loss: 0.3661\n",
      "Epoch 335/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3713 - val_loss: 0.3654\n",
      "Epoch 336/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3715 - val_loss: 0.3645\n",
      "Epoch 337/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3711 - val_loss: 0.3640\n",
      "Epoch 338/600\n",
      "2559/2559 [==============================] - 0s 78us/sample - loss: 0.3713 - val_loss: 0.3624\n",
      "Epoch 339/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.3720 - val_loss: 0.3669\n",
      "Epoch 340/600\n",
      "2559/2559 [==============================] - 0s 86us/sample - loss: 0.3704 - val_loss: 0.3662\n",
      "Epoch 341/600\n",
      "2559/2559 [==============================] - 0s 100us/sample - loss: 0.3703 - val_loss: 0.3656\n",
      "Epoch 342/600\n",
      "2559/2559 [==============================] - 0s 99us/sample - loss: 0.3695 - val_loss: 0.3641\n",
      "Epoch 343/600\n",
      "2559/2559 [==============================] - 0s 101us/sample - loss: 0.3702 - val_loss: 0.3629\n",
      "Epoch 344/600\n",
      "2559/2559 [==============================] - 0s 78us/sample - loss: 0.3713 - val_loss: 0.3640\n",
      "Epoch 345/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3710 - val_loss: 0.3666\n",
      "Epoch 346/600\n",
      "2559/2559 [==============================] - 0s 80us/sample - loss: 0.3705 - val_loss: 0.3624\n",
      "Epoch 347/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3705 - val_loss: 0.3648\n",
      "Epoch 348/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3702 - val_loss: 0.3684\n",
      "Epoch 349/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.3700 - val_loss: 0.3637\n",
      "Epoch 350/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3709 - val_loss: 0.3662\n",
      "Epoch 351/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3699 - val_loss: 0.3697\n",
      "Epoch 352/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3722 - val_loss: 0.3639\n",
      "Epoch 353/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3698 - val_loss: 0.3664\n",
      "Epoch 354/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3709 - val_loss: 0.3676\n",
      "Epoch 355/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3714 - val_loss: 0.3660\n",
      "Epoch 356/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3714 - val_loss: 0.3661\n",
      "Epoch 357/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3690 - val_loss: 0.3649\n",
      "Epoch 358/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3715 - val_loss: 0.3634\n",
      "Epoch 359/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3697 - val_loss: 0.3678\n",
      "Epoch 360/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3708 - val_loss: 0.3660\n",
      "Epoch 361/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3697 - val_loss: 0.3678\n",
      "Epoch 362/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3730 - val_loss: 0.3672\n",
      "Epoch 363/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3693 - val_loss: 0.3649\n",
      "Epoch 364/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3692 - val_loss: 0.3641\n",
      "Epoch 365/600\n",
      "2559/2559 [==============================] - 0s 69us/sample - loss: 0.3693 - val_loss: 0.3656\n",
      "Epoch 366/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3698 - val_loss: 0.3652\n",
      "Epoch 367/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3706 - val_loss: 0.3647\n",
      "Epoch 368/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3690 - val_loss: 0.3660\n",
      "Epoch 369/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3704 - val_loss: 0.3636\n",
      "Epoch 370/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3701 - val_loss: 0.3656\n",
      "Epoch 371/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3707 - val_loss: 0.3676\n",
      "Epoch 372/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3692 - val_loss: 0.3635\n",
      "Epoch 373/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3699 - val_loss: 0.3684\n",
      "Epoch 374/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3699 - val_loss: 0.3674\n",
      "Epoch 375/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3700 - val_loss: 0.3648\n",
      "Epoch 376/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3699 - val_loss: 0.3715\n",
      "Epoch 377/600\n",
      "2559/2559 [==============================] - 0s 77us/sample - loss: 0.3703 - val_loss: 0.3649\n",
      "Epoch 378/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3691 - val_loss: 0.3641\n",
      "Epoch 379/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3690 - val_loss: 0.3739\n",
      "Epoch 380/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.3692 - val_loss: 0.3688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 381/600\n",
      "2559/2559 [==============================] - 0s 63us/sample - loss: 0.3687 - val_loss: 0.3703\n",
      "Epoch 382/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3689 - val_loss: 0.3648\n",
      "Epoch 383/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3702 - val_loss: 0.3645\n",
      "Epoch 384/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3710 - val_loss: 0.3649\n",
      "Epoch 385/600\n",
      "2559/2559 [==============================] - 0s 69us/sample - loss: 0.3703 - val_loss: 0.3638\n",
      "Epoch 386/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3705 - val_loss: 0.3664\n",
      "Epoch 387/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3693 - val_loss: 0.3647\n",
      "Epoch 388/600\n",
      "2559/2559 [==============================] - 0s 86us/sample - loss: 0.3689 - val_loss: 0.3695\n",
      "Epoch 389/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.3698 - val_loss: 0.3656\n",
      "Epoch 390/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3691 - val_loss: 0.3668\n",
      "Epoch 391/600\n",
      "2559/2559 [==============================] - 0s 77us/sample - loss: 0.3682 - val_loss: 0.3666\n",
      "Epoch 392/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3688 - val_loss: 0.3666\n",
      "Epoch 393/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3694 - val_loss: 0.3702\n",
      "Epoch 394/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3688 - val_loss: 0.3710\n",
      "Epoch 395/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3696 - val_loss: 0.3639\n",
      "Epoch 396/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3692 - val_loss: 0.3685\n",
      "Epoch 397/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3679 - val_loss: 0.3715\n",
      "Epoch 398/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3686 - val_loss: 0.3693\n",
      "Epoch 399/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3677 - val_loss: 0.3685\n",
      "Epoch 400/600\n",
      "2559/2559 [==============================] - 0s 77us/sample - loss: 0.3681 - val_loss: 0.3673\n",
      "Epoch 401/600\n",
      "2559/2559 [==============================] - 0s 88us/sample - loss: 0.3703 - val_loss: 0.3639\n",
      "Epoch 402/600\n",
      "2559/2559 [==============================] - 0s 82us/sample - loss: 0.3678 - val_loss: 0.3665\n",
      "Epoch 403/600\n",
      "2559/2559 [==============================] - 0s 118us/sample - loss: 0.3687 - val_loss: 0.3682\n",
      "Epoch 404/600\n",
      "2559/2559 [==============================] - 0s 83us/sample - loss: 0.3681 - val_loss: 0.3701\n",
      "Epoch 405/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3698 - val_loss: 0.3654\n",
      "Epoch 406/600\n",
      "2559/2559 [==============================] - 0s 89us/sample - loss: 0.3693 - val_loss: 0.3685\n",
      "Epoch 407/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3684 - val_loss: 0.3669\n",
      "Epoch 408/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3687 - val_loss: 0.3701\n",
      "Epoch 409/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3680 - val_loss: 0.3687\n",
      "Epoch 410/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3679 - val_loss: 0.3661\n",
      "Epoch 411/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3686 - val_loss: 0.3689\n",
      "Epoch 412/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3687 - val_loss: 0.3680\n",
      "Epoch 413/600\n",
      "2559/2559 [==============================] - 0s 77us/sample - loss: 0.3681 - val_loss: 0.3694\n",
      "Epoch 414/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3685 - val_loss: 0.3684\n",
      "Epoch 415/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3679 - val_loss: 0.3692\n",
      "Epoch 416/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3673 - val_loss: 0.3683\n",
      "Epoch 417/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.3682 - val_loss: 0.3692\n",
      "Epoch 418/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3679 - val_loss: 0.3669\n",
      "Epoch 419/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3673 - val_loss: 0.3706\n",
      "Epoch 420/600\n",
      "2559/2559 [==============================] - 0s 69us/sample - loss: 0.3668 - val_loss: 0.3719\n",
      "Epoch 421/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3682 - val_loss: 0.3683\n",
      "Epoch 422/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3686 - val_loss: 0.3665\n",
      "Epoch 423/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3682 - val_loss: 0.3699\n",
      "Epoch 424/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3693 - val_loss: 0.3671\n",
      "Epoch 425/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3672 - val_loss: 0.3725\n",
      "Epoch 426/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3686 - val_loss: 0.3666\n",
      "Epoch 427/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3689 - val_loss: 0.3667\n",
      "Epoch 428/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3677 - val_loss: 0.3676\n",
      "Epoch 429/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3679 - val_loss: 0.3692\n",
      "Epoch 430/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3684 - val_loss: 0.3665\n",
      "Epoch 431/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3678 - val_loss: 0.3703\n",
      "Epoch 432/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3670 - val_loss: 0.3662\n",
      "Epoch 433/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3687 - val_loss: 0.3737\n",
      "Epoch 434/600\n",
      "2559/2559 [==============================] - 0s 79us/sample - loss: 0.3677 - val_loss: 0.3696\n",
      "Epoch 435/600\n",
      "2559/2559 [==============================] - 0s 77us/sample - loss: 0.3680 - val_loss: 0.3671\n",
      "Epoch 436/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3672 - val_loss: 0.3675\n",
      "Epoch 437/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3666 - val_loss: 0.3722\n",
      "Epoch 438/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.3698 - val_loss: 0.3678\n",
      "Epoch 439/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3670 - val_loss: 0.3692\n",
      "Epoch 440/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3694 - val_loss: 0.3672\n",
      "Epoch 441/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3677 - val_loss: 0.3678\n",
      "Epoch 442/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3675 - val_loss: 0.3662\n",
      "Epoch 443/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3673 - val_loss: 0.3692\n",
      "Epoch 444/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3676 - val_loss: 0.3715\n",
      "Epoch 445/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3681 - val_loss: 0.3704\n",
      "Epoch 446/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3668 - val_loss: 0.3702\n",
      "Epoch 447/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3673 - val_loss: 0.3713\n",
      "Epoch 448/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3662 - val_loss: 0.3686\n",
      "Epoch 449/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3679 - val_loss: 0.3719\n",
      "Epoch 450/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3671 - val_loss: 0.3677\n",
      "Epoch 451/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3674 - val_loss: 0.3710\n",
      "Epoch 452/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3664 - val_loss: 0.3699\n",
      "Epoch 453/600\n",
      "2559/2559 [==============================] - 0s 78us/sample - loss: 0.3676 - val_loss: 0.3702\n",
      "Epoch 454/600\n",
      "2559/2559 [==============================] - 0s 86us/sample - loss: 0.3670 - val_loss: 0.3686\n",
      "Epoch 455/600\n",
      "2559/2559 [==============================] - 0s 80us/sample - loss: 0.3677 - val_loss: 0.3693\n",
      "Epoch 456/600\n",
      "2559/2559 [==============================] - 0s 82us/sample - loss: 0.3665 - val_loss: 0.3762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 457/600\n",
      "2559/2559 [==============================] - 0s 120us/sample - loss: 0.3671 - val_loss: 0.3688\n",
      "Epoch 458/600\n",
      "2559/2559 [==============================] - 0s 81us/sample - loss: 0.3671 - val_loss: 0.3717\n",
      "Epoch 459/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3675 - val_loss: 0.3708\n",
      "Epoch 460/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3669 - val_loss: 0.3717\n",
      "Epoch 461/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3691 - val_loss: 0.3676\n",
      "Epoch 462/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3670 - val_loss: 0.3693\n",
      "Epoch 463/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3671 - val_loss: 0.3698\n",
      "Epoch 464/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3682 - val_loss: 0.3717\n",
      "Epoch 465/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3671 - val_loss: 0.3695\n",
      "Epoch 466/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3675 - val_loss: 0.3740\n",
      "Epoch 467/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3670 - val_loss: 0.3744\n",
      "Epoch 468/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3668 - val_loss: 0.3708\n",
      "Epoch 469/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3659 - val_loss: 0.3670\n",
      "Epoch 470/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3662 - val_loss: 0.3758\n",
      "Epoch 471/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3664 - val_loss: 0.3704\n",
      "Epoch 472/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.3689 - val_loss: 0.3708\n",
      "Epoch 473/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3678 - val_loss: 0.3702\n",
      "Epoch 474/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3678 - val_loss: 0.3684\n",
      "Epoch 475/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3678 - val_loss: 0.3707\n",
      "Epoch 476/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3680 - val_loss: 0.3691\n",
      "Epoch 477/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3654 - val_loss: 0.3659\n",
      "Epoch 478/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3669 - val_loss: 0.3714\n",
      "Epoch 479/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3668 - val_loss: 0.3669\n",
      "Epoch 480/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3671 - val_loss: 0.3689\n",
      "Epoch 481/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3663 - val_loss: 0.3692\n",
      "Epoch 482/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3665 - val_loss: 0.3711\n",
      "Epoch 483/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.3660 - val_loss: 0.3693\n",
      "Epoch 484/600\n",
      "2559/2559 [==============================] - 0s 63us/sample - loss: 0.3653 - val_loss: 0.3677\n",
      "Epoch 485/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3660 - val_loss: 0.3696\n",
      "Epoch 486/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.3655 - val_loss: 0.3683\n",
      "Epoch 487/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3663 - val_loss: 0.3707\n",
      "Epoch 488/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3658 - val_loss: 0.3710\n",
      "Epoch 489/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3661 - val_loss: 0.3698\n",
      "Epoch 490/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3672 - val_loss: 0.3729\n",
      "Epoch 491/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3665 - val_loss: 0.3737\n",
      "Epoch 492/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3659 - val_loss: 0.3721\n",
      "Epoch 493/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3662 - val_loss: 0.3709\n",
      "Epoch 494/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.3653 - val_loss: 0.3714\n",
      "Epoch 495/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3667 - val_loss: 0.3709\n",
      "Epoch 496/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3670 - val_loss: 0.3699\n",
      "Epoch 497/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3661 - val_loss: 0.3743\n",
      "Epoch 498/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3669 - val_loss: 0.3694\n",
      "Epoch 499/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3655 - val_loss: 0.3712\n",
      "Epoch 500/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3655 - val_loss: 0.3678\n",
      "Epoch 501/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3670 - val_loss: 0.3689\n",
      "Epoch 502/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3652 - val_loss: 0.3700\n",
      "Epoch 503/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3659 - val_loss: 0.3704\n",
      "Epoch 504/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3651 - val_loss: 0.3734\n",
      "Epoch 505/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3657 - val_loss: 0.3732\n",
      "Epoch 506/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3664 - val_loss: 0.3700\n",
      "Epoch 507/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3662 - val_loss: 0.3681\n",
      "Epoch 508/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3664 - val_loss: 0.3718\n",
      "Epoch 509/600\n",
      "2559/2559 [==============================] - 0s 62us/sample - loss: 0.3672 - val_loss: 0.3779\n",
      "Epoch 510/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3659 - val_loss: 0.3732\n",
      "Epoch 511/600\n",
      "2559/2559 [==============================] - 0s 78us/sample - loss: 0.3661 - val_loss: 0.3710\n",
      "Epoch 512/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3654 - val_loss: 0.3708\n",
      "Epoch 513/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3647 - val_loss: 0.3727\n",
      "Epoch 514/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.3662 - val_loss: 0.3731\n",
      "Epoch 515/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3654 - val_loss: 0.3695\n",
      "Epoch 516/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3659 - val_loss: 0.3730\n",
      "Epoch 517/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3650 - val_loss: 0.3732\n",
      "Epoch 518/600\n",
      "2559/2559 [==============================] - 0s 69us/sample - loss: 0.3661 - val_loss: 0.3720\n",
      "Epoch 519/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3650 - val_loss: 0.3676\n",
      "Epoch 520/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3696 - val_loss: 0.3733\n",
      "Epoch 521/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3647 - val_loss: 0.3742\n",
      "Epoch 522/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3663 - val_loss: 0.3740\n",
      "Epoch 523/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3640 - val_loss: 0.3815\n",
      "Epoch 524/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3661 - val_loss: 0.3673\n",
      "Epoch 525/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3652 - val_loss: 0.3723\n",
      "Epoch 526/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3675 - val_loss: 0.3777\n",
      "Epoch 527/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3669 - val_loss: 0.3732\n",
      "Epoch 528/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.3642 - val_loss: 0.3737\n",
      "Epoch 529/600\n",
      "2559/2559 [==============================] - 0s 101us/sample - loss: 0.3645 - val_loss: 0.3703\n",
      "Epoch 530/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3645 - val_loss: 0.3769\n",
      "Epoch 531/600\n",
      "2559/2559 [==============================] - 0s 77us/sample - loss: 0.3652 - val_loss: 0.3736\n",
      "Epoch 532/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.3641 - val_loss: 0.3702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 533/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3655 - val_loss: 0.3719\n",
      "Epoch 534/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3646 - val_loss: 0.3698\n",
      "Epoch 535/600\n",
      "2559/2559 [==============================] - 0s 88us/sample - loss: 0.3650 - val_loss: 0.3846\n",
      "Epoch 536/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3647 - val_loss: 0.3733\n",
      "Epoch 537/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3651 - val_loss: 0.3788\n",
      "Epoch 538/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3640 - val_loss: 0.3776\n",
      "Epoch 539/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3654 - val_loss: 0.3765\n",
      "Epoch 540/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3653 - val_loss: 0.3761\n",
      "Epoch 541/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3663 - val_loss: 0.3737\n",
      "Epoch 542/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3639 - val_loss: 0.3750\n",
      "Epoch 543/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.3638 - val_loss: 0.3713\n",
      "Epoch 544/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3673 - val_loss: 0.3760\n",
      "Epoch 545/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3647 - val_loss: 0.3721\n",
      "Epoch 546/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3635 - val_loss: 0.3725\n",
      "Epoch 547/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3640 - val_loss: 0.3736\n",
      "Epoch 548/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3630 - val_loss: 0.3726\n",
      "Epoch 549/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3646 - val_loss: 0.3763\n",
      "Epoch 550/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3638 - val_loss: 0.3744\n",
      "Epoch 551/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3635 - val_loss: 0.3711\n",
      "Epoch 552/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3641 - val_loss: 0.3840\n",
      "Epoch 553/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3654 - val_loss: 0.3753\n",
      "Epoch 554/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3639 - val_loss: 0.3748\n",
      "Epoch 555/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.3643 - val_loss: 0.3747\n",
      "Epoch 556/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3637 - val_loss: 0.3750\n",
      "Epoch 557/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3636 - val_loss: 0.3719\n",
      "Epoch 558/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3659 - val_loss: 0.3745\n",
      "Epoch 559/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3630 - val_loss: 0.3725\n",
      "Epoch 560/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3629 - val_loss: 0.3702\n",
      "Epoch 561/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3639 - val_loss: 0.3774\n",
      "Epoch 562/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3655 - val_loss: 0.3811\n",
      "Epoch 563/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3633 - val_loss: 0.3770\n",
      "Epoch 564/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3648 - val_loss: 0.3687\n",
      "Epoch 565/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3657 - val_loss: 0.3748\n",
      "Epoch 566/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3653 - val_loss: 0.3742\n",
      "Epoch 567/600\n",
      "2559/2559 [==============================] - 0s 79us/sample - loss: 0.3640 - val_loss: 0.3727\n",
      "Epoch 568/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3645 - val_loss: 0.3739\n",
      "Epoch 569/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3640 - val_loss: 0.3761\n",
      "Epoch 570/600\n",
      "2559/2559 [==============================] - 0s 86us/sample - loss: 0.3642 - val_loss: 0.3770\n",
      "Epoch 571/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3634 - val_loss: 0.3764\n",
      "Epoch 572/600\n",
      "2559/2559 [==============================] - 0s 67us/sample - loss: 0.3625 - val_loss: 0.3746\n",
      "Epoch 573/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3627 - val_loss: 0.3768\n",
      "Epoch 574/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3635 - val_loss: 0.3732\n",
      "Epoch 575/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3627 - val_loss: 0.3725\n",
      "Epoch 576/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.3646 - val_loss: 0.3766\n",
      "Epoch 577/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.3646 - val_loss: 0.3733\n",
      "Epoch 578/600\n",
      "2559/2559 [==============================] - 0s 64us/sample - loss: 0.3628 - val_loss: 0.3740\n",
      "Epoch 579/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3628 - val_loss: 0.3766\n",
      "Epoch 580/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3662 - val_loss: 0.3803\n",
      "Epoch 581/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3630 - val_loss: 0.3787\n",
      "Epoch 582/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.3659 - val_loss: 0.3773\n",
      "Epoch 583/600\n",
      "2559/2559 [==============================] - 0s 69us/sample - loss: 0.3633 - val_loss: 0.3741\n",
      "Epoch 584/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3634 - val_loss: 0.3772\n",
      "Epoch 585/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.3626 - val_loss: 0.3806\n",
      "Epoch 586/600\n",
      "2559/2559 [==============================] - 0s 81us/sample - loss: 0.3662 - val_loss: 0.3740\n",
      "Epoch 587/600\n",
      "2559/2559 [==============================] - 0s 80us/sample - loss: 0.3633 - val_loss: 0.3755\n",
      "Epoch 588/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3629 - val_loss: 0.3745\n",
      "Epoch 589/600\n",
      "2559/2559 [==============================] - 0s 79us/sample - loss: 0.3636 - val_loss: 0.3805\n",
      "Epoch 590/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3626 - val_loss: 0.3778\n",
      "Epoch 591/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.3619 - val_loss: 0.3787\n",
      "Epoch 592/600\n",
      "2559/2559 [==============================] - 0s 70us/sample - loss: 0.3625 - val_loss: 0.3755\n",
      "Epoch 593/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3634 - val_loss: 0.3776\n",
      "Epoch 594/600\n",
      "2559/2559 [==============================] - 0s 81us/sample - loss: 0.3627 - val_loss: 0.3732\n",
      "Epoch 595/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3633 - val_loss: 0.3755\n",
      "Epoch 596/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.3637 - val_loss: 0.3732\n",
      "Epoch 597/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3633 - val_loss: 0.3771\n",
      "Epoch 598/600\n",
      "2559/2559 [==============================] - 0s 80us/sample - loss: 0.3621 - val_loss: 0.3746\n",
      "Epoch 599/600\n",
      "2559/2559 [==============================] - 0s 65us/sample - loss: 0.3632 - val_loss: 0.3771\n",
      "Epoch 600/600\n",
      "2559/2559 [==============================] - 0s 71us/sample - loss: 0.3635 - val_loss: 0.3764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a44d6a400>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the Model\n",
    "\n",
    "model.fit(x=X_train,y=y_train,epochs=600,validation_data=(X_test, y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a44daeb38>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss = pd.DataFrame(model.history.history)\n",
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92       943\n",
      "           1       0.41      0.14      0.20       154\n",
      "\n",
      "    accuracy                           0.85      1097\n",
      "   macro avg       0.64      0.55      0.56      1097\n",
      "weighted avg       0.81      0.85      0.82      1097\n",
      "\n",
      "[[913  30]\n",
      " [133  21]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "nn_pred = model.predict_classes(X_test)\n",
    "print(classification_report(y_test,nn_pred))\n",
    "print(confusion_matrix(y_test,nn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding DropOut Layers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=15,activation='relu'))#,kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=7,activation='relu'))#, kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=3,activation='relu'))#, kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wait 25 epochs after monitoring validation loss\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2559 samples, validate on 1097 samples\n",
      "Epoch 1/600\n",
      "2559/2559 [==============================] - 1s 469us/sample - loss: 0.5928 - val_loss: 0.4813\n",
      "Epoch 2/600\n",
      "2559/2559 [==============================] - 0s 84us/sample - loss: 0.5097 - val_loss: 0.4406\n",
      "Epoch 3/600\n",
      "2559/2559 [==============================] - 0s 108us/sample - loss: 0.4882 - val_loss: 0.4280\n",
      "Epoch 4/600\n",
      "2559/2559 [==============================] - 0s 85us/sample - loss: 0.4728 - val_loss: 0.4088\n",
      "Epoch 5/600\n",
      "2559/2559 [==============================] - 0s 85us/sample - loss: 0.4653 - val_loss: 0.4040\n",
      "Epoch 6/600\n",
      "2559/2559 [==============================] - 0s 82us/sample - loss: 0.4531 - val_loss: 0.3966\n",
      "Epoch 7/600\n",
      "2559/2559 [==============================] - 0s 77us/sample - loss: 0.4539 - val_loss: 0.3927\n",
      "Epoch 8/600\n",
      "2559/2559 [==============================] - 0s 84us/sample - loss: 0.4419 - val_loss: 0.3872\n",
      "Epoch 9/600\n",
      "2559/2559 [==============================] - 0s 86us/sample - loss: 0.4362 - val_loss: 0.3836\n",
      "Epoch 10/600\n",
      "2559/2559 [==============================] - 0s 84us/sample - loss: 0.4339 - val_loss: 0.3812\n",
      "Epoch 11/600\n",
      "2559/2559 [==============================] - 0s 85us/sample - loss: 0.4318 - val_loss: 0.3786\n",
      "Epoch 12/600\n",
      "2559/2559 [==============================] - 0s 83us/sample - loss: 0.4250 - val_loss: 0.3760\n",
      "Epoch 13/600\n",
      "2559/2559 [==============================] - 0s 92us/sample - loss: 0.4286 - val_loss: 0.3759\n",
      "Epoch 14/600\n",
      "2559/2559 [==============================] - 0s 87us/sample - loss: 0.4218 - val_loss: 0.3733\n",
      "Epoch 15/600\n",
      "2559/2559 [==============================] - 0s 84us/sample - loss: 0.4232 - val_loss: 0.3731\n",
      "Epoch 16/600\n",
      "2559/2559 [==============================] - 0s 85us/sample - loss: 0.4265 - val_loss: 0.3737\n",
      "Epoch 17/600\n",
      "2559/2559 [==============================] - 0s 93us/sample - loss: 0.4187 - val_loss: 0.3701\n",
      "Epoch 18/600\n",
      "2559/2559 [==============================] - 0s 88us/sample - loss: 0.4250 - val_loss: 0.3700\n",
      "Epoch 19/600\n",
      "2559/2559 [==============================] - 0s 88us/sample - loss: 0.4171 - val_loss: 0.3724\n",
      "Epoch 20/600\n",
      "2559/2559 [==============================] - 0s 85us/sample - loss: 0.4170 - val_loss: 0.3663\n",
      "Epoch 21/600\n",
      "2559/2559 [==============================] - 0s 84us/sample - loss: 0.4158 - val_loss: 0.3656\n",
      "Epoch 22/600\n",
      "2559/2559 [==============================] - 0s 86us/sample - loss: 0.4195 - val_loss: 0.3665\n",
      "Epoch 23/600\n",
      "2559/2559 [==============================] - 0s 84us/sample - loss: 0.4154 - val_loss: 0.3655\n",
      "Epoch 24/600\n",
      "2559/2559 [==============================] - 0s 89us/sample - loss: 0.4159 - val_loss: 0.3663\n",
      "Epoch 25/600\n",
      "2559/2559 [==============================] - 0s 86us/sample - loss: 0.4149 - val_loss: 0.3630\n",
      "Epoch 26/600\n",
      "2559/2559 [==============================] - 0s 86us/sample - loss: 0.4119 - val_loss: 0.3634\n",
      "Epoch 27/600\n",
      "2559/2559 [==============================] - 0s 83us/sample - loss: 0.4223 - val_loss: 0.3672\n",
      "Epoch 28/600\n",
      "2559/2559 [==============================] - 0s 86us/sample - loss: 0.4082 - val_loss: 0.3653\n",
      "Epoch 29/600\n",
      "2559/2559 [==============================] - 0s 85us/sample - loss: 0.4093 - val_loss: 0.3631\n",
      "Epoch 30/600\n",
      "2559/2559 [==============================] - 0s 93us/sample - loss: 0.4107 - val_loss: 0.3649\n",
      "Epoch 31/600\n",
      "2559/2559 [==============================] - 0s 90us/sample - loss: 0.4183 - val_loss: 0.3649\n",
      "Epoch 32/600\n",
      "2559/2559 [==============================] - 0s 99us/sample - loss: 0.4161 - val_loss: 0.3668\n",
      "Epoch 33/600\n",
      "2559/2559 [==============================] - 0s 86us/sample - loss: 0.4098 - val_loss: 0.3633\n",
      "Epoch 34/600\n",
      "2559/2559 [==============================] - 0s 88us/sample - loss: 0.4140 - val_loss: 0.3643\n",
      "Epoch 35/600\n",
      "2559/2559 [==============================] - 0s 138us/sample - loss: 0.4097 - val_loss: 0.3634\n",
      "Epoch 36/600\n",
      "2559/2559 [==============================] - 0s 101us/sample - loss: 0.4136 - val_loss: 0.3629\n",
      "Epoch 37/600\n",
      "2559/2559 [==============================] - 0s 138us/sample - loss: 0.4085 - val_loss: 0.3610\n",
      "Epoch 38/600\n",
      "2559/2559 [==============================] - 0s 109us/sample - loss: 0.4091 - val_loss: 0.3635\n",
      "Epoch 39/600\n",
      "2559/2559 [==============================] - 0s 89us/sample - loss: 0.4087 - val_loss: 0.3618\n",
      "Epoch 40/600\n",
      "2559/2559 [==============================] - 0s 93us/sample - loss: 0.4086 - val_loss: 0.3617\n",
      "Epoch 41/600\n",
      "2559/2559 [==============================] - 0s 95us/sample - loss: 0.4085 - val_loss: 0.3606\n",
      "Epoch 42/600\n",
      "2559/2559 [==============================] - 0s 85us/sample - loss: 0.4079 - val_loss: 0.3630\n",
      "Epoch 43/600\n",
      "2559/2559 [==============================] - 0s 86us/sample - loss: 0.4094 - val_loss: 0.3611\n",
      "Epoch 44/600\n",
      "2559/2559 [==============================] - 0s 84us/sample - loss: 0.4073 - val_loss: 0.3633\n",
      "Epoch 45/600\n",
      "2559/2559 [==============================] - 0s 85us/sample - loss: 0.4050 - val_loss: 0.3616\n",
      "Epoch 46/600\n",
      "2559/2559 [==============================] - 0s 98us/sample - loss: 0.4072 - val_loss: 0.3644\n",
      "Epoch 47/600\n",
      "2559/2559 [==============================] - 0s 88us/sample - loss: 0.4085 - val_loss: 0.3610\n",
      "Epoch 48/600\n",
      "2559/2559 [==============================] - 0s 86us/sample - loss: 0.4060 - val_loss: 0.3614\n",
      "Epoch 49/600\n",
      "2559/2559 [==============================] - 0s 84us/sample - loss: 0.4034 - val_loss: 0.3607\n",
      "Epoch 50/600\n",
      "2559/2559 [==============================] - 0s 83us/sample - loss: 0.4058 - val_loss: 0.3611\n",
      "Epoch 51/600\n",
      "2559/2559 [==============================] - 0s 85us/sample - loss: 0.4043 - val_loss: 0.3598\n",
      "Epoch 52/600\n",
      "2559/2559 [==============================] - 0s 84us/sample - loss: 0.4086 - val_loss: 0.3602\n",
      "Epoch 53/600\n",
      "2559/2559 [==============================] - 0s 102us/sample - loss: 0.4032 - val_loss: 0.3597\n",
      "Epoch 54/600\n",
      "2559/2559 [==============================] - 0s 80us/sample - loss: 0.4014 - val_loss: 0.3621\n",
      "Epoch 55/600\n",
      "2559/2559 [==============================] - 0s 78us/sample - loss: 0.4032 - val_loss: 0.3601\n",
      "Epoch 56/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.4016 - val_loss: 0.3587\n",
      "Epoch 57/600\n",
      "2559/2559 [==============================] - 0s 76us/sample - loss: 0.4023 - val_loss: 0.3610\n",
      "Epoch 58/600\n",
      "2559/2559 [==============================] - 0s 81us/sample - loss: 0.4007 - val_loss: 0.3608\n",
      "Epoch 59/600\n",
      "2559/2559 [==============================] - 0s 101us/sample - loss: 0.3993 - val_loss: 0.3589\n",
      "Epoch 60/600\n",
      "2559/2559 [==============================] - 0s 80us/sample - loss: 0.3987 - val_loss: 0.3611\n",
      "Epoch 61/600\n",
      "2559/2559 [==============================] - 0s 79us/sample - loss: 0.3966 - val_loss: 0.3592\n",
      "Epoch 62/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.4019 - val_loss: 0.3583\n",
      "Epoch 63/600\n",
      "2559/2559 [==============================] - 0s 87us/sample - loss: 0.4043 - val_loss: 0.3590\n",
      "Epoch 64/600\n",
      "2559/2559 [==============================] - 0s 93us/sample - loss: 0.3992 - val_loss: 0.3598\n",
      "Epoch 65/600\n",
      "2559/2559 [==============================] - 0s 94us/sample - loss: 0.4004 - val_loss: 0.3594\n",
      "Epoch 66/600\n",
      "2559/2559 [==============================] - 0s 89us/sample - loss: 0.4028 - val_loss: 0.3604\n",
      "Epoch 67/600\n",
      "2559/2559 [==============================] - 0s 94us/sample - loss: 0.4051 - val_loss: 0.3609\n",
      "Epoch 68/600\n",
      "2559/2559 [==============================] - 0s 89us/sample - loss: 0.4001 - val_loss: 0.3608\n",
      "Epoch 69/600\n",
      "2559/2559 [==============================] - 0s 82us/sample - loss: 0.4032 - val_loss: 0.3609\n",
      "Epoch 70/600\n",
      "2559/2559 [==============================] - 0s 73us/sample - loss: 0.4024 - val_loss: 0.3589\n",
      "Epoch 71/600\n",
      "2559/2559 [==============================] - 0s 86us/sample - loss: 0.4002 - val_loss: 0.3616\n",
      "Epoch 72/600\n",
      "2559/2559 [==============================] - 0s 82us/sample - loss: 0.4013 - val_loss: 0.3608\n",
      "Epoch 73/600\n",
      "2559/2559 [==============================] - 0s 85us/sample - loss: 0.3996 - val_loss: 0.3637\n",
      "Epoch 74/600\n",
      "2559/2559 [==============================] - 0s 81us/sample - loss: 0.4014 - val_loss: 0.3601\n",
      "Epoch 75/600\n",
      "2559/2559 [==============================] - 0s 78us/sample - loss: 0.4020 - val_loss: 0.3625\n",
      "Epoch 76/600\n",
      "2559/2559 [==============================] - 0s 78us/sample - loss: 0.4007 - val_loss: 0.3605\n",
      "Epoch 77/600\n",
      "2559/2559 [==============================] - 0s 83us/sample - loss: 0.3997 - val_loss: 0.3620\n",
      "Epoch 78/600\n",
      "2559/2559 [==============================] - 0s 77us/sample - loss: 0.4036 - val_loss: 0.3621\n",
      "Epoch 79/600\n",
      "2559/2559 [==============================] - 0s 66us/sample - loss: 0.4000 - val_loss: 0.3600\n",
      "Epoch 80/600\n",
      "2559/2559 [==============================] - 0s 72us/sample - loss: 0.4020 - val_loss: 0.3612\n",
      "Epoch 81/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.4043 - val_loss: 0.3617\n",
      "Epoch 82/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.3975 - val_loss: 0.3602\n",
      "Epoch 83/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.3973 - val_loss: 0.3618\n",
      "Epoch 84/600\n",
      "2559/2559 [==============================] - 0s 75us/sample - loss: 0.4059 - val_loss: 0.3618\n",
      "Epoch 85/600\n",
      "2559/2559 [==============================] - 0s 77us/sample - loss: 0.4003 - val_loss: 0.3624\n",
      "Epoch 86/600\n",
      "2559/2559 [==============================] - 0s 74us/sample - loss: 0.3999 - val_loss: 0.3606\n",
      "Epoch 87/600\n",
      "2559/2559 [==============================] - 0s 68us/sample - loss: 0.4060 - val_loss: 0.3628\n",
      "Epoch 00087: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a4595ef60>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=600, validation_data=(X_test, y_test), verbose=1,callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a4598de10>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss = pd.DataFrame(model.history.history)\n",
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92       943\n",
      "           1       0.00      0.00      0.00       154\n",
      "\n",
      "    accuracy                           0.86      1097\n",
      "   macro avg       0.43      0.50      0.46      1097\n",
      "weighted avg       0.74      0.86      0.79      1097\n",
      "\n",
      "[[943   0]\n",
      " [154   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaydip/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "nn_pred = model.predict_classes(X_test)\n",
    "print(classification_report(y_test,nn_pred))\n",
    "print(confusion_matrix(y_test,nn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K Nearest Neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train,y_train)\n",
    "knn_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.88       943\n",
      "           1       0.26      0.27      0.26       154\n",
      "\n",
      "    accuracy                           0.79      1097\n",
      "   macro avg       0.57      0.57      0.57      1097\n",
      "weighted avg       0.79      0.79      0.79      1097\n",
      "\n",
      "[[824 119]\n",
      " [113  41]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,knn_pred))\n",
    "print(confusion_matrix(y_test,knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing k value\n",
    "\n",
    "error_rate = []\n",
    "for i in range(1,40):\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Error Rate')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,40),error_rate,color='blue',marker='o')\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[937   6]\n",
      " [151   3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92       943\n",
      "           1       0.33      0.02      0.04       154\n",
      "\n",
      "    accuracy                           0.86      1097\n",
      "   macro avg       0.60      0.51      0.48      1097\n",
      "weighted avg       0.79      0.86      0.80      1097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#with k=12\n",
    "knn = KNeighborsClassifier(n_neighbors=12)\n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "pred_k = knn.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,pred_k))\n",
    "print(classification_report(y_test,pred_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train,y_train)\n",
    "dtree_pred = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87       943\n",
      "           1       0.23      0.26      0.25       154\n",
      "\n",
      "    accuracy                           0.78      1097\n",
      "   macro avg       0.56      0.56      0.56      1097\n",
      "weighted avg       0.79      0.78      0.78      1097\n",
      "\n",
      "[[812 131]\n",
      " [114  40]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,dtree_pred))\n",
    "print(confusion_matrix(y_test,dtree_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the Random Forest model\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=600)\n",
    "rfc.fit(X_train,y_train)\n",
    "rfc_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92       943\n",
      "           1       0.44      0.05      0.09       154\n",
      "\n",
      "    accuracy                           0.86      1097\n",
      "   macro avg       0.65      0.52      0.51      1097\n",
      "weighted avg       0.81      0.86      0.81      1097\n",
      "\n",
      "[[933  10]\n",
      " [146   8]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,rfc_pred))\n",
    "print(confusion_matrix(y_test,rfc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train,y_train)\n",
    "svc_pred = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92       943\n",
      "           1       0.50      0.01      0.03       154\n",
      "\n",
      "    accuracy                           0.86      1097\n",
      "   macro avg       0.68      0.51      0.47      1097\n",
      "weighted avg       0.81      0.86      0.80      1097\n",
      "\n",
      "[[941   2]\n",
      " [152   2]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,svc_pred))\n",
    "print(confusion_matrix(y_test,svc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] C=0.1, gamma=1 ..................................................\n",
      "[CV] ................................... C=0.1, gamma=1, total=   0.1s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n",
      "[CV] ................................... C=0.1, gamma=1, total=   0.1s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................... C=0.1, gamma=1, total=   0.1s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n",
      "[CV] ................................... C=0.1, gamma=1, total=   0.1s\n",
      "[CV] C=0.1, gamma=1 ..................................................\n",
      "[CV] ................................... C=0.1, gamma=1, total=   0.1s\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV] ................................. C=0.1, gamma=0.1, total=   0.1s\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV] ................................. C=0.1, gamma=0.1, total=   0.1s\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV] ................................. C=0.1, gamma=0.1, total=   0.1s\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV] ................................. C=0.1, gamma=0.1, total=   0.1s\n",
      "[CV] C=0.1, gamma=0.1 ................................................\n",
      "[CV] ................................. C=0.1, gamma=0.1, total=   0.1s\n",
      "[CV] C=0.1, gamma=0.01 ...............................................\n",
      "[CV] ................................ C=0.1, gamma=0.01, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.01 ...............................................\n",
      "[CV] ................................ C=0.1, gamma=0.01, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.01 ...............................................\n",
      "[CV] ................................ C=0.1, gamma=0.01, total=   0.1s\n",
      "[CV] C=0.1, gamma=0.01 ...............................................\n",
      "[CV] ................................ C=0.1, gamma=0.01, total=   0.1s\n",
      "[CV] C=0.1, gamma=0.01 ...............................................\n",
      "[CV] ................................ C=0.1, gamma=0.01, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.001 ..............................................\n",
      "[CV] ............................... C=0.1, gamma=0.001, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.001 ..............................................\n",
      "[CV] ............................... C=0.1, gamma=0.001, total=   0.1s\n",
      "[CV] C=0.1, gamma=0.001 ..............................................\n",
      "[CV] ............................... C=0.1, gamma=0.001, total=   0.1s\n",
      "[CV] C=0.1, gamma=0.001 ..............................................\n",
      "[CV] ............................... C=0.1, gamma=0.001, total=   0.1s\n",
      "[CV] C=0.1, gamma=0.001 ..............................................\n",
      "[CV] ............................... C=0.1, gamma=0.001, total=   0.0s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   0.1s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   0.1s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   0.1s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   0.1s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   0.1s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total=   0.1s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total=   0.1s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total=   0.1s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total=   0.1s\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total=   0.1s\n",
      "[CV] C=1, gamma=0.01 .................................................\n",
      "[CV] .................................. C=1, gamma=0.01, total=   0.1s\n",
      "[CV] C=1, gamma=0.01 .................................................\n",
      "[CV] .................................. C=1, gamma=0.01, total=   0.1s\n",
      "[CV] C=1, gamma=0.01 .................................................\n",
      "[CV] .................................. C=1, gamma=0.01, total=   0.1s\n",
      "[CV] C=1, gamma=0.01 .................................................\n",
      "[CV] .................................. C=1, gamma=0.01, total=   0.1s\n",
      "[CV] C=1, gamma=0.01 .................................................\n",
      "[CV] .................................. C=1, gamma=0.01, total=   0.1s\n",
      "[CV] C=1, gamma=0.001 ................................................\n",
      "[CV] ................................. C=1, gamma=0.001, total=   0.0s\n",
      "[CV] C=1, gamma=0.001 ................................................\n",
      "[CV] ................................. C=1, gamma=0.001, total=   0.0s\n",
      "[CV] C=1, gamma=0.001 ................................................\n",
      "[CV] ................................. C=1, gamma=0.001, total=   0.0s\n",
      "[CV] C=1, gamma=0.001 ................................................\n",
      "[CV] ................................. C=1, gamma=0.001, total=   0.1s\n",
      "[CV] C=1, gamma=0.001 ................................................\n",
      "[CV] ................................. C=1, gamma=0.001, total=   0.1s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   0.2s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   0.2s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   0.2s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   0.1s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   0.2s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total=   0.1s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total=   0.1s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total=   0.1s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total=   0.2s\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total=   0.2s\n",
      "[CV] C=10, gamma=0.01 ................................................\n",
      "[CV] ................................. C=10, gamma=0.01, total=   0.1s\n",
      "[CV] C=10, gamma=0.01 ................................................\n",
      "[CV] ................................. C=10, gamma=0.01, total=   0.1s\n",
      "[CV] C=10, gamma=0.01 ................................................\n",
      "[CV] ................................. C=10, gamma=0.01, total=   0.1s\n",
      "[CV] C=10, gamma=0.01 ................................................\n",
      "[CV] ................................. C=10, gamma=0.01, total=   0.1s\n",
      "[CV] C=10, gamma=0.01 ................................................\n",
      "[CV] ................................. C=10, gamma=0.01, total=   0.1s\n",
      "[CV] C=10, gamma=0.001 ...............................................\n",
      "[CV] ................................ C=10, gamma=0.001, total=   0.1s\n",
      "[CV] C=10, gamma=0.001 ...............................................\n",
      "[CV] ................................ C=10, gamma=0.001, total=   0.0s\n",
      "[CV] C=10, gamma=0.001 ...............................................\n",
      "[CV] ................................ C=10, gamma=0.001, total=   0.0s\n",
      "[CV] C=10, gamma=0.001 ...............................................\n",
      "[CV] ................................ C=10, gamma=0.001, total=   0.0s\n",
      "[CV] C=10, gamma=0.001 ...............................................\n",
      "[CV] ................................ C=10, gamma=0.001, total=   0.1s\n",
      "[CV] C=100, gamma=1 ..................................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................... C=100, gamma=1, total=   0.3s\n",
      "[CV] C=100, gamma=1 ..................................................\n",
      "[CV] ................................... C=100, gamma=1, total=   0.3s\n",
      "[CV] C=100, gamma=1 ..................................................\n",
      "[CV] ................................... C=100, gamma=1, total=   0.3s\n",
      "[CV] C=100, gamma=1 ..................................................\n",
      "[CV] ................................... C=100, gamma=1, total=   0.3s\n",
      "[CV] C=100, gamma=1 ..................................................\n",
      "[CV] ................................... C=100, gamma=1, total=   0.3s\n",
      "[CV] C=100, gamma=0.1 ................................................\n",
      "[CV] ................................. C=100, gamma=0.1, total=   0.4s\n",
      "[CV] C=100, gamma=0.1 ................................................\n",
      "[CV] ................................. C=100, gamma=0.1, total=   0.3s\n",
      "[CV] C=100, gamma=0.1 ................................................\n",
      "[CV] ................................. C=100, gamma=0.1, total=   0.3s\n",
      "[CV] C=100, gamma=0.1 ................................................\n",
      "[CV] ................................. C=100, gamma=0.1, total=   0.3s\n",
      "[CV] C=100, gamma=0.1 ................................................\n",
      "[CV] ................................. C=100, gamma=0.1, total=   0.3s\n",
      "[CV] C=100, gamma=0.01 ...............................................\n",
      "[CV] ................................ C=100, gamma=0.01, total=   0.3s\n",
      "[CV] C=100, gamma=0.01 ...............................................\n",
      "[CV] ................................ C=100, gamma=0.01, total=   0.2s\n",
      "[CV] C=100, gamma=0.01 ...............................................\n",
      "[CV] ................................ C=100, gamma=0.01, total=   0.2s\n",
      "[CV] C=100, gamma=0.01 ...............................................\n",
      "[CV] ................................ C=100, gamma=0.01, total=   0.3s\n",
      "[CV] C=100, gamma=0.01 ...............................................\n",
      "[CV] ................................ C=100, gamma=0.01, total=   0.3s\n",
      "[CV] C=100, gamma=0.001 ..............................................\n",
      "[CV] ............................... C=100, gamma=0.001, total=   0.1s\n",
      "[CV] C=100, gamma=0.001 ..............................................\n",
      "[CV] ............................... C=100, gamma=0.001, total=   0.1s\n",
      "[CV] C=100, gamma=0.001 ..............................................\n",
      "[CV] ............................... C=100, gamma=0.001, total=   0.1s\n",
      "[CV] C=100, gamma=0.001 ..............................................\n",
      "[CV] ............................... C=100, gamma=0.001, total=   0.1s\n",
      "[CV] C=100, gamma=0.001 ..............................................\n",
      "[CV] ............................... C=100, gamma=0.001, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:   10.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'C': [0.1, 1, 10, 100],\n",
       "                         'gamma': [1, 0.1, 0.01, 0.001]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gridsearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001]} \n",
    "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_pred = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92       943\n",
      "           1       0.39      0.07      0.12       154\n",
      "\n",
      "    accuracy                           0.85      1097\n",
      "   macro avg       0.63      0.53      0.52      1097\n",
      "weighted avg       0.80      0.85      0.81      1097\n",
      "\n",
      "[[926  17]\n",
      " [143  11]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,grid_pred))\n",
    "print(confusion_matrix(y_test,grid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train,y_train)\n",
    "log_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93       943\n",
      "           1       0.75      0.06      0.11       154\n",
      "\n",
      "    accuracy                           0.87      1097\n",
      "   macro avg       0.81      0.53      0.52      1097\n",
      "weighted avg       0.85      0.87      0.81      1097\n",
      "\n",
      "[[940   3]\n",
      " [145   9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(classification_report(y_test,log_pred))\n",
    "print(confusion_matrix(y_test,log_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
